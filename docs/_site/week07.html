<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="A comprehensive educational resource for learning Artificial Intelligence - structured 14-week curriculum for undergraduate students">
  
  <title>Week 7: Machine Learning Fundamentals | AI Learning Students</title>
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <!-- Custom styles -->
  <link rel="stylesheet" href="/ai-learning-students/assets/css/style.css">
</head>
<body>
  <div class="page-wrapper">
    <!-- Navigation Header -->
    <header class="site-header">
      <nav class="nav-container">
        <div class="nav-brand">
          <a href="/ai-learning-students/" class="brand-link">
            <div class="brand-icon">
              <i class="fas fa-brain"></i>
            </div>
            <span class="brand-text">AI Learning Students</span>
          </a>
        </div>
        
        <div class="nav-menu" id="navMenu">
          <div class="nav-links">
            <a href="/ai-learning-students/" class="nav-link ">
              <i class="fas fa-home"></i>
              Home
            </a>
            <div class="nav-dropdown">
              <a href="#" class="nav-link dropdown-toggle">
                <i class="fas fa-book"></i>
                Weeks
                <i class="fas fa-chevron-down"></i>
              </a>
              <div class="dropdown-menu">
                
                  
                  <a href="/ai-learning-students/week01.html" class="dropdown-link">
                    Week 1
                  </a>
                
                  
                  <a href="/ai-learning-students/week02.html" class="dropdown-link">
                    Week 2
                  </a>
                
                  
                  <a href="/ai-learning-students/week03.html" class="dropdown-link">
                    Week 3
                  </a>
                
                  
                  <a href="/ai-learning-students/week04.html" class="dropdown-link">
                    Week 4
                  </a>
                
                  
                  <a href="/ai-learning-students/week05.html" class="dropdown-link">
                    Week 5
                  </a>
                
                  
                  <a href="/ai-learning-students/week06.html" class="dropdown-link">
                    Week 6
                  </a>
                
                  
                  <a href="/ai-learning-students/week07.html" class="dropdown-link">
                    Week 7
                  </a>
                
                  
                  <a href="/ai-learning-students/week08.html" class="dropdown-link">
                    Week 8
                  </a>
                
                  
                  <a href="/ai-learning-students/week09.html" class="dropdown-link">
                    Week 9
                  </a>
                
                  
                  <a href="/ai-learning-students/week10.html" class="dropdown-link">
                    Week 10
                  </a>
                
                  
                  <a href="/ai-learning-students/week11.html" class="dropdown-link">
                    Week 11
                  </a>
                
                  
                  <a href="/ai-learning-students/week12.html" class="dropdown-link">
                    Week 12
                  </a>
                
                  
                  <a href="/ai-learning-students/week13.html" class="dropdown-link">
                    Week 13
                  </a>
                
                  
                  <a href="/ai-learning-students/week14.html" class="dropdown-link">
                    Week 14
                  </a>
                
              </div>
            </div>
          </div>
          
          <div class="nav-progress">
            <div class="progress-indicator">
              <div class="progress-circle">
                <div class="progress-fill"></div>
                <span class="progress-text">0%</span>
              </div>
            </div>
          </div>
        </div>
        
        <button class="mobile-menu-toggle" id="mobileToggle">
          <span></span>
          <span></span>
          <span></span>
        </button>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main-content">
      <div class="content-container">
        <div class="week-header">
  <div class="week-hero">
    <div class="week-number-large"></div>
    <div class="week-title-section">
      <h1>Week 7: Machine Learning Fundamentals</h1>
      <div class="week-progress-bar">
        <div class="progress-fill" style="width: 0.0%"></div>
      </div>
    </div>
  </div>
  
  <div class="week-navigation">
    
    
    
    <a href="index.html" class="nav-button home">
      <i class="fas fa-home"></i>
      Course Overview
    </a>
    
    
      <a href="week01.html" class="nav-button next">
        Next Week
        <i class="fas fa-chevron-right"></i>
      </a>
    
  </div>
</div>

<div class="week-content">
  <h3 id="navigation">Navigation</h3>
<p><a href="index.html">üè† Home</a> | <a href="week06.html">‚Üê Previous: Week 6</a> | <a href="week08.html">Next: Week 8 ‚Üí</a></p>

<hr />

<h1 id="week-7-machine-learning-fundamentals">Week 7: Machine Learning Fundamentals</h1>

<h2 id="-learning-objectives">üìö Learning Objectives</h2>
<p>By the end of this week, students will be able to:</p>
<ul>
  <li>Understand the core concepts and motivation behind machine learning</li>
  <li>Distinguish between supervised, unsupervised, and reinforcement learning</li>
  <li>Explain the machine learning workflow and data pipeline</li>
  <li>Understand key concepts: overfitting, underfitting, bias-variance tradeoff</li>
  <li>Implement basic gradient descent and apply it to simple problems</li>
</ul>

<hr />

<h2 id="-what-is-machine-learning">ü§ñ What is Machine Learning?</h2>

<p><strong>Machine Learning (ML)</strong> is the field of study that gives computers the ability to learn patterns from data without being explicitly programmed for every scenario.</p>

<p><img src="images/machine-learning-overview.png" alt="ML Overview" /></p>

<h3 id="traditional-programming-vs-machine-learning">Traditional Programming vs. Machine Learning</h3>

<table>
  <thead>
    <tr>
      <th>Traditional Programming</th>
      <th>Machine Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Data + Program ‚Üí Output</td>
      <td>Data + Output ‚Üí Program</td>
    </tr>
    <tr>
      <td>Rules are hand-coded</td>
      <td>Rules are learned</td>
    </tr>
    <tr>
      <td>Limited to known scenarios</td>
      <td>Generalizes to new scenarios</td>
    </tr>
    <tr>
      <td>Deterministic behavior</td>
      <td>Probabilistic predictions</td>
    </tr>
  </tbody>
</table>

<h3 id="why-machine-learning-now">Why Machine Learning Now?</h3>

<ol>
  <li><strong>Big Data:</strong> Massive datasets available (internet, sensors, digital records)</li>
  <li><strong>Computing Power:</strong> GPUs, cloud computing, distributed systems</li>
  <li><strong>Better Algorithms:</strong> Deep learning, ensemble methods, optimization</li>
  <li><strong>Real-world Success:</strong> Image recognition, translation, recommendations</li>
</ol>

<h3 id="key-insight-pattern-recognition">Key Insight: Pattern Recognition</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Traditional approach: Hard-coded rules
</span><span class="k">def</span> <span class="nf">detect_spam_traditional</span><span class="p">(</span><span class="n">email</span><span class="p">):</span>
    <span class="n">spam_words</span> <span class="o">=</span> <span class="p">[</span><span class="s">'free'</span><span class="p">,</span> <span class="s">'money'</span><span class="p">,</span> <span class="s">'click'</span><span class="p">,</span> <span class="s">'buy'</span><span class="p">,</span> <span class="s">'discount'</span><span class="p">]</span>
    <span class="n">spam_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">spam_words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">email</span><span class="p">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">spam_count</span> <span class="o">&gt;=</span> <span class="mi">2</span>

<span class="c1"># Machine learning approach: Learn from examples
</span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>

<span class="k">def</span> <span class="nf">train_spam_detector</span><span class="p">(</span><span class="n">emails</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="s">"""Learn spam detection from examples"""</span>
    <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">emails</span><span class="p">)</span>
    
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
    <span class="n">classifier</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">classifier</span>

<span class="k">def</span> <span class="nf">predict_spam_ml</span><span class="p">(</span><span class="n">email</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">classifier</span><span class="p">):</span>
    <span class="s">"""Use learned model to classify new email"""</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">transform</span><span class="p">([</span><span class="n">email</span><span class="p">])</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">confidence</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">confidence</span>

<span class="c1"># Example usage
</span><span class="n">emails</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Free money click now!"</span><span class="p">,</span> <span class="s">"Meeting at 3pm today"</span><span class="p">,</span> <span class="s">"Buy discount pills"</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">"spam"</span><span class="p">,</span> <span class="s">"not spam"</span><span class="p">,</span> <span class="s">"spam"</span><span class="p">]</span>

<span class="n">vectorizer</span><span class="p">,</span> <span class="n">classifier</span> <span class="o">=</span> <span class="n">train_spam_detector</span><span class="p">(</span><span class="n">emails</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">prediction</span><span class="p">,</span> <span class="n">confidence</span> <span class="o">=</span> <span class="n">predict_spam_ml</span><span class="p">(</span><span class="s">"Free discount offer"</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">classifier</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Prediction: </span><span class="si">{</span><span class="n">prediction</span><span class="si">}</span><span class="s"> (confidence: </span><span class="si">{</span><span class="n">confidence</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="-types-of-machine-learning">üìä Types of Machine Learning</h2>

<h3 id="1-supervised-learning">1. Supervised Learning</h3>
<p><strong>Goal:</strong> Learn a mapping from inputs to outputs using labeled training data</p>

<p><strong>Examples:</strong></p>
<ul>
  <li><strong>Classification:</strong> Email spam detection, image recognition, medical diagnosis</li>
  <li><strong>Regression:</strong> House price prediction, stock forecasting, temperature prediction</li>
</ul>

<p><strong>Training Data Format:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (features) ‚Üí Output (labels)
[house_size, location, age] ‚Üí price
[pixel_values] ‚Üí cat/dog
[email_text] ‚Üí spam/not_spam
</code></pre></div></div>

<p><img src="images/supervised-learning-example.png" alt="Supervised Learning" /></p>

<h3 id="2-unsupervised-learning">2. Unsupervised Learning</h3>
<p><strong>Goal:</strong> Find hidden patterns or structure in data without labeled examples</p>

<p><strong>Examples:</strong></p>
<ul>
  <li><strong>Clustering:</strong> Customer segmentation, gene sequencing, market research</li>
  <li><strong>Dimensionality Reduction:</strong> Data visualization, compression, feature extraction</li>
  <li><strong>Anomaly Detection:</strong> Fraud detection, network intrusion, quality control</li>
</ul>

<p><strong>Training Data Format:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (features only) ‚Üí Find patterns
[customer_purchases] ‚Üí customer_groups
[document_words] ‚Üí topic_categories  
[network_traffic] ‚Üí normal/anomalous
</code></pre></div></div>

<h3 id="3-reinforcement-learning">3. Reinforcement Learning</h3>
<p><strong>Goal:</strong> Learn optimal actions through interaction with environment and rewards</p>

<p><strong>Examples:</strong></p>
<ul>
  <li><strong>Game Playing:</strong> Chess, Go, video games</li>
  <li><strong>Robotics:</strong> Navigation, manipulation, autonomous vehicles</li>
  <li><strong>Resource Management:</strong> Traffic control, energy optimization</li>
</ul>

<p><strong>Learning Process:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>State ‚Üí Action ‚Üí Reward ‚Üí New State
Environment provides feedback through rewards/penalties
Agent learns policy to maximize long-term reward
</code></pre></div></div>

<p><img src="images/reinforcement-learning-loop.png" alt="RL Loop" /></p>

<hr />

<h2 id="-the-machine-learning-workflow">üîÑ The Machine Learning Workflow</h2>

<h3 id="1-problem-definition">1. Problem Definition</h3>
<ul>
  <li><strong>What are we trying to predict?</strong> (target variable)</li>
  <li><strong>What type of ML problem?</strong> (classification, regression, clustering)</li>
  <li><strong>What constitutes success?</strong> (accuracy, precision, speed)</li>
  <li><strong>What constraints exist?</strong> (time, interpretability, data privacy)</li>
</ul>

<h3 id="2-data-collection--exploration">2. Data Collection &amp; Exploration</h3>
<ul>
  <li><strong>Gather relevant data</strong> from various sources</li>
  <li><strong>Explore data characteristics:</strong> size, types, distributions</li>
  <li><strong>Identify data quality issues:</strong> missing values, outliers, errors</li>
  <li><strong>Understand domain context</strong> and feature meanings</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="k">def</span> <span class="nf">explore_data</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="s">"""Basic data exploration"""</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Dataset shape:"</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Data types:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">dtypes</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Missing values:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">())</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Basic statistics:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">describe</span><span class="p">())</span>
    
    <span class="c1"># Visualize distributions
</span>    <span class="n">numeric_columns</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'float64'</span><span class="p">,</span> <span class="s">'int64'</span><span class="p">]).</span><span class="n">columns</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">column</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">numeric_columns</span><span class="p">[:</span><span class="mi">6</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Distribution of </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Frequency'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example usage with housing data
# df = pd.read_csv('housing_data.csv')
# explore_data(df)
</span></code></pre></div></div>

<h3 id="3-data-preprocessing">3. Data Preprocessing</h3>
<ul>
  <li><strong>Clean the data:</strong> handle missing values, remove duplicates</li>
  <li><strong>Feature engineering:</strong> create new features, transform existing ones</li>
  <li><strong>Data encoding:</strong> convert categorical variables to numbers</li>
  <li><strong>Normalization/Scaling:</strong> ensure features have similar scales</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>

<span class="k">def</span> <span class="nf">preprocess_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">numeric_features</span><span class="p">,</span> <span class="n">categorical_features</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="s">"""Complete preprocessing pipeline"""</span>
    
    <span class="c1"># Separate features and target
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">numeric_features</span> <span class="o">+</span> <span class="n">categorical_features</span><span class="p">].</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">target</span><span class="p">].</span><span class="n">copy</span><span class="p">()</span>
    
    <span class="c1"># Create preprocessing pipelines
</span>    <span class="n">numeric_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s">'imputer'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'median'</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">'scaler'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())</span>
    <span class="p">])</span>
    
    <span class="n">categorical_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s">'imputer'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="s">'missing'</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">'onehot'</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="s">'first'</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
    <span class="p">])</span>
    
    <span class="c1"># Combine preprocessing steps
</span>    <span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
        <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
            <span class="p">(</span><span class="s">'num'</span><span class="p">,</span> <span class="n">numeric_transformer</span><span class="p">,</span> <span class="n">numeric_features</span><span class="p">),</span>
            <span class="p">(</span><span class="s">'cat'</span><span class="p">,</span> <span class="n">categorical_transformer</span><span class="p">,</span> <span class="n">categorical_features</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">preprocessor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</code></pre></div></div>

<h3 id="4-model-selection--training">4. Model Selection &amp; Training</h3>
<ul>
  <li><strong>Choose appropriate algorithms</strong> based on problem type and data</li>
  <li><strong>Split data:</strong> training, validation, test sets</li>
  <li><strong>Train models</strong> using training data</li>
  <li><strong>Tune hyperparameters</strong> using validation data</li>
</ul>

<h3 id="5-model-evaluation">5. Model Evaluation</h3>
<ul>
  <li><strong>Assess performance</strong> using test data</li>
  <li><strong>Compare different models</strong> and approaches</li>
  <li><strong>Analyze errors</strong> and failure cases</li>
  <li><strong>Validate assumptions</strong> about data and model</li>
</ul>

<h3 id="6-deployment--monitoring">6. Deployment &amp; Monitoring</h3>
<ul>
  <li><strong>Deploy model</strong> to production environment</li>
  <li><strong>Monitor performance</strong> over time</li>
  <li><strong>Retrain periodically</strong> as new data becomes available</li>
  <li><strong>Handle concept drift</strong> when data patterns change</li>
</ul>

<p><img src="images/ml-workflow-diagram.png" alt="ML Workflow" /></p>

<hr />

<h2 id="-key-machine-learning-concepts">üìà Key Machine Learning Concepts</h2>

<h3 id="loss-functions">Loss Functions</h3>
<p><strong>Loss function</strong> measures how wrong our model‚Äôs predictions are:</p>

<h4 id="regression-loss-functions">Regression Loss Functions</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="s">"""MSE - penalizes large errors heavily"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="s">"""MAE - robust to outliers"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">huber_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="s">"""Huber - combines MSE and MAE benefits"""</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">condition</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">&lt;=</span> <span class="n">delta</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> 
                <span class="mf">0.5</span> <span class="o">*</span> <span class="n">residual</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="n">residual</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">delta</span><span class="p">))</span>
    <span class="p">)</span>

<span class="c1"># Example comparison
</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>  <span class="c1"># Note the outlier
</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"MSE: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"MAE: </span><span class="si">{</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>  
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Huber: </span><span class="si">{</span><span class="n">huber_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="classification-loss-functions">Classification Loss Functions</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">zero_one_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="s">"""0-1 Loss (accuracy-based)"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span> <span class="o">!=</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">):</span>
    <span class="s">"""Cross-entropy for probabilistic predictions"""</span>
    <span class="c1"># Add small epsilon to prevent log(0)
</span>    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-15</span>
    <span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred_proba</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred_proba</span><span class="p">)</span> <span class="o">+</span> 
                   <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred_proba</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="s">"""Hinge loss for SVM"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="gradient-descent">Gradient Descent</h3>
<p><strong>Core optimization algorithm</strong> that iteratively minimizes the loss function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">:</span>
    <span class="s">"""Gradient Descent implementation with different variants"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_epochs</span> <span class="o">=</span> <span class="n">max_epochs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tolerance</span> <span class="o">=</span> <span class="n">tolerance</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">batch_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Standard batch gradient descent"""</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_epochs</span><span class="p">):</span>
            <span class="c1"># Forward pass
</span>            <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
            
            <span class="c1"># Compute gradients
</span>            <span class="n">gradients</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
            
            <span class="c1"># Update parameters
</span>            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>
            
            <span class="c1"># Check convergence
</span>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">tolerance</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Converged after </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s"> epochs"</span><span class="p">)</span>
                    <span class="k">break</span>
        
        <span class="k">return</span> <span class="n">theta</span>
    
    <span class="k">def</span> <span class="nf">stochastic_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Stochastic gradient descent (SGD)"""</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_epochs</span><span class="p">):</span>
            <span class="c1"># Shuffle the data
</span>            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
            <span class="n">X_shuffled</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
            <span class="n">y_shuffled</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
            
            <span class="n">epoch_cost</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
                <span class="c1"># Use single example
</span>                <span class="n">xi</span> <span class="o">=</span> <span class="n">X_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">yi</span> <span class="o">=</span> <span class="n">y_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                
                <span class="c1"># Forward pass
</span>                <span class="n">prediction</span> <span class="o">=</span> <span class="n">xi</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
                <span class="n">cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">yi</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="n">epoch_cost</span> <span class="o">+=</span> <span class="n">cost</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                
                <span class="c1"># Compute gradients
</span>                <span class="n">gradient</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xi</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">yi</span><span class="p">)</span>
                
                <span class="c1"># Update parameters
</span>                <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span>
            
            <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_cost</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">theta</span>
    
    <span class="k">def</span> <span class="nf">mini_batch_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="s">"""Mini-batch gradient descent"""</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_epochs</span><span class="p">):</span>
            <span class="c1"># Shuffle the data
</span>            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
            <span class="n">X_shuffled</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
            <span class="n">y_shuffled</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
            
            <span class="n">epoch_cost</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">num_batches</span> <span class="o">=</span> <span class="n">m</span> <span class="o">//</span> <span class="n">batch_size</span>
            
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
                <span class="c1"># Get mini-batch
</span>                <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
                <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
                <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_shuffled</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
                
                <span class="c1"># Forward pass
</span>                <span class="n">predictions</span> <span class="o">=</span> <span class="n">X_batch</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
                <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y_batch</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
                <span class="n">epoch_cost</span> <span class="o">+=</span> <span class="n">cost</span>
                
                <span class="c1"># Compute gradients
</span>                <span class="n">batch_m</span> <span class="o">=</span> <span class="n">X_batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">gradients</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">batch_m</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_batch</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y_batch</span><span class="p">)</span>
                
                <span class="c1"># Update parameters
</span>                <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>
            
            <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_cost</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">theta</span>
    
    <span class="k">def</span> <span class="nf">plot_convergence</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""Plot the cost function over time"""</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Cost Function During Training'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Cost'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example usage
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">true_theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">true_theta</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">gd</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">learned_theta</span> <span class="o">=</span> <span class="n">gd</span><span class="p">.</span><span class="n">batch_gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"True parameters: </span><span class="si">{</span><span class="n">true_theta</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Learned parameters: </span><span class="si">{</span><span class="n">learned_theta</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">true_theta</span> <span class="o">-</span> <span class="n">learned_theta</span><span class="p">):.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="Ô∏è-the-bias-variance-tradeoff">‚öñÔ∏è The Bias-Variance Tradeoff</h2>

<h3 id="understanding-model-complexity">Understanding Model Complexity</h3>

<p><img src="images/bias-variance-tradeoff.png" alt="Bias-Variance" /></p>

<h4 id="high-bias-underfitting">High Bias (Underfitting)</h4>
<ul>
  <li><strong>Model is too simple</strong> to capture underlying patterns</li>
  <li><strong>Poor performance</strong> on both training and test data</li>
  <li><strong>Examples:</strong> Linear model for non-linear data</li>
  <li><strong>Solutions:</strong> Use more complex model, add features</li>
</ul>

<h4 id="high-variance-overfitting">High Variance (Overfitting)</h4>
<ul>
  <li><strong>Model is too complex</strong> and memorizes training noise</li>
  <li><strong>Good training performance, poor test performance</strong></li>
  <li><strong>Examples:</strong> Deep neural network on small dataset</li>
  <li><strong>Solutions:</strong> More data, regularization, simpler model</li>
</ul>

<h4 id="the-sweet-spot">The Sweet Spot</h4>
<ul>
  <li><strong>Balance between bias and variance</strong></li>
  <li><strong>Good generalization</strong> to unseen data</li>
  <li><strong>Achieved through:</strong> Cross-validation, regularization, ensemble methods</li>
</ul>

<h3 id="detecting-overfittingunderfitting">Detecting Overfitting/Underfitting</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">learning_curve</span>

<span class="k">def</span> <span class="nf">plot_learning_curves</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="s">"""Plot learning curves to diagnose bias/variance"""</span>
    
    <span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">val_scores</span> <span class="o">=</span> <span class="n">learning_curve</span><span class="p">(</span>
        <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> 
        <span class="n">train_sizes</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
        <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span>
    <span class="p">)</span>
    
    <span class="c1"># Convert to positive values
</span>    <span class="n">train_scores</span> <span class="o">=</span> <span class="o">-</span><span class="n">train_scores</span>
    <span class="n">val_scores</span> <span class="o">=</span> <span class="o">-</span><span class="n">val_scores</span>
    
    <span class="n">train_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">train_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">val_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">val_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_mean</span> <span class="o">-</span> <span class="n">train_std</span><span class="p">,</span> 
                     <span class="n">train_mean</span> <span class="o">+</span> <span class="n">train_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">val_mean</span> <span class="o">-</span> <span class="n">val_std</span><span class="p">,</span>
                     <span class="n">val_mean</span> <span class="o">+</span> <span class="n">val_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_mean</span><span class="p">,</span> <span class="s">'o-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training Score'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">val_mean</span><span class="p">,</span> <span class="s">'o-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation Score'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Training Set Size'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Mean Squared Error'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Learning Curves'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="c1"># Interpretation guide
</span>    <span class="n">final_gap</span> <span class="o">=</span> <span class="n">val_mean</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">train_mean</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">final_gap</span> <span class="o">&gt;</span> <span class="mf">0.1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"‚ö†Ô∏è  High variance (overfitting): Large gap between curves"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"   Solutions: More data, regularization, simpler model"</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">train_mean</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># Assuming normalized error
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"‚ö†Ô∏è  High bias (underfitting): Both curves plateau high"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"   Solutions: More complex model, more features"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"‚úÖ Good fit: Curves converge at reasonable error level"</span><span class="p">)</span>

<span class="c1"># Example with polynomial regression
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># Generate synthetic data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Different complexity models
</span><span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'Linear (degree=1)'</span><span class="p">:</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s">'poly'</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">]),</span>
    <span class="s">'Quadratic (degree=2)'</span><span class="p">:</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s">'poly'</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span> 
        <span class="p">(</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">]),</span>
    <span class="s">'High-degree (degree=15)'</span><span class="p">:</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s">'poly'</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
        <span class="p">(</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">])</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">:"</span><span class="p">)</span>
    <span class="n">plot_learning_curves</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="-training-validation-and-test-sets">üîÑ Training, Validation, and Test Sets</h2>

<h3 id="the-three-way-split">The Three-Way Split</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split data into train/temp (80/20)
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_temp</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span>  <span class="c1"># stratify for classification
</span><span class="p">)</span>

<span class="c1"># Split temp into validation/test (10/10 of original)
</span><span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_temp</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_temp</span>
<span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Training set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s"> samples"</span><span class="p">)</span>     <span class="c1"># 80%
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Validation set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span><span class="si">}</span><span class="s"> samples"</span><span class="p">)</span>     <span class="c1"># 10%  
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Test set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s"> samples"</span><span class="p">)</span>          <span class="c1"># 10%
</span></code></pre></div></div>

<h3 id="purpose-of-each-set">Purpose of Each Set</h3>
<ul>
  <li><strong>Training Set (60-80%):</strong> Learn model parameters</li>
  <li><strong>Validation Set (10-20%):</strong> Tune hyperparameters, model selection</li>
  <li><strong>Test Set (10-20%):</strong> Final unbiased performance estimate</li>
</ul>

<h3 id="cross-validation">Cross-Validation</h3>
<p><strong>Alternative to validation set</strong> for small datasets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="k">def</span> <span class="nf">comprehensive_cross_validation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">problem_type</span><span class="o">=</span><span class="s">'classification'</span><span class="p">):</span>
    <span class="s">"""Perform multiple types of cross-validation"""</span>
    
    <span class="k">if</span> <span class="n">problem_type</span> <span class="o">==</span> <span class="s">'classification'</span><span class="p">:</span>
        <span class="n">cv_strategies</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'Stratified 5-Fold'</span><span class="p">:</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
            <span class="s">'Standard 5-Fold'</span><span class="p">:</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
            <span class="s">'Stratified 10-Fold'</span><span class="p">:</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="s">'accuracy'</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cv_strategies</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'Standard 5-Fold'</span><span class="p">:</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
            <span class="s">'Standard 10-Fold'</span><span class="p">:</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="s">'neg_mean_squared_error'</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">cv</span> <span class="ow">in</span> <span class="n">cv_strategies</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">scoring</span> <span class="o">==</span> <span class="s">'neg_mean_squared_error'</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="o">-</span><span class="n">scores</span>  <span class="c1"># Convert to positive RMSE
</span>            <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        
        <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'mean'</span><span class="p">:</span> <span class="n">scores</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span>
            <span class="s">'std'</span><span class="p">:</span> <span class="n">scores</span><span class="p">.</span><span class="n">std</span><span class="p">(),</span>
            <span class="s">'scores'</span><span class="p">:</span> <span class="n">scores</span>
        <span class="p">}</span>
        
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">scores</span><span class="p">.</span><span class="n">mean</span><span class="p">():.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> (+/- </span><span class="si">{</span><span class="n">scores</span><span class="p">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Example usage
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">comprehensive_cross_validation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="-hands-on-exercise-linear-regression-from-scratch">üíª Hands-On Exercise: Linear Regression from Scratch</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="k">class</span> <span class="nc">LinearRegressionFromScratch</span><span class="p">:</span>
    <span class="s">"""Complete linear regression implementation with visualization"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_epochs</span> <span class="o">=</span> <span class="n">max_epochs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tolerance</span> <span class="o">=</span> <span class="n">tolerance</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight_history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">_add_bias_term</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""Add bias column to feature matrix"""</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'gradient_descent'</span><span class="p">):</span>
        <span class="s">"""Fit the model using specified method"""</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'gradient_descent'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_fit_gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'normal_equation'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_fit_normal_equation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Method must be 'gradient_descent' or 'normal_equation'"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_fit_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Fit using gradient descent"""</span>
        <span class="c1"># Initialize parameters
</span>        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">X_with_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_add_bias_term</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># +1 for bias
</span>        
        <span class="n">prev_cost</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_epochs</span><span class="p">):</span>
            <span class="c1"># Forward pass
</span>            <span class="n">predictions</span> <span class="o">=</span> <span class="n">X_with_bias</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
            
            <span class="c1"># Compute cost
</span>            <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">weight_history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            
            <span class="c1"># Compute gradients
</span>            <span class="n">gradients</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">X_with_bias</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
            
            <span class="c1"># Update parameters
</span>            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>
            
            <span class="c1"># Check convergence
</span>            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">prev_cost</span> <span class="o">-</span> <span class="n">cost</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">tolerance</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Converged after </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s"> epochs"</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="n">prev_cost</span> <span class="o">=</span> <span class="n">cost</span>
        
        <span class="c1"># Store final parameters
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    
    <span class="k">def</span> <span class="nf">_fit_normal_equation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Fit using normal equation (closed form solution)"""</span>
        <span class="n">X_with_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_add_bias_term</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="c1"># Normal equation: Œ∏ = (X^T X)^(-1) X^T y
</span>        <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_with_bias</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_with_bias</span><span class="p">)).</span><span class="n">dot</span><span class="p">(</span><span class="n">X_with_bias</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        
        <span class="c1"># Calculate final cost for comparison
</span>        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">final_cost</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">final_cost</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""Make predictions on new data"""</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Calculate R-squared score"""</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">ss_res</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">ss_tot</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">ss_res</span> <span class="o">/</span> <span class="n">ss_tot</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">plot_training_progress</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""Visualize training progress"""</span>
        <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        
        <span class="c1"># Plot cost history
</span>        <span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cost_history</span><span class="p">)</span>
        <span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Training Cost Over Time'</span><span class="p">)</span>
        <span class="n">ax1</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
        <span class="n">ax1</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Mean Squared Error'</span><span class="p">)</span>
        <span class="n">ax1</span><span class="p">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
        <span class="n">ax1</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="c1"># Plot weight evolution (for 1D case)
</span>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_history</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_history</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">weight_history</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_history</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weight_history</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">label</span> <span class="o">=</span> <span class="s">'bias'</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sa">f</span><span class="s">'weight_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">'</span>
                <span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">weight_history</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
            <span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Parameter Evolution'</span><span class="p">)</span>
            <span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
            <span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Parameter Value'</span><span class="p">)</span>
            <span class="n">ax2</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">ax2</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">plot_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_test</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""Plot predictions vs actual (for 1D features)"""</span>
        <span class="k">if</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Plotting only available for 1D features"</span><span class="p">)</span>
            <span class="k">return</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        
        <span class="c1"># Training data and predictions
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training Data'</span><span class="p">)</span>
        
        <span class="c1"># Sort for smooth line
</span>        <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span> <span class="s">'r-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Predictions'</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Feature Value'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Target Value'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Training Data (R¬≤ = </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">)'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="c1"># Test data if provided
</span>        <span class="k">if</span> <span class="n">X_test</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">y_test</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">test_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
            
            <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Test Data'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'orange'</span><span class="p">)</span>
            <span class="n">sort_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">test_predictions</span><span class="p">[</span><span class="n">sort_idx</span><span class="p">],</span> <span class="s">'g-'</span><span class="p">,</span> 
                    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Predictions'</span><span class="p">)</span>
            
            <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Feature Value'</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Target Value'</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Test Data (R¬≤ = </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">)'</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Comprehensive example
</span><span class="k">def</span> <span class="nf">demonstrate_linear_regression</span><span class="p">():</span>
    <span class="s">"""Complete demonstration of linear regression"""</span>
    
    <span class="c1"># Generate synthetic dataset
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Split the data
</span>    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
    <span class="p">)</span>
    
    <span class="c1"># Scale the features
</span>    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"=== Linear Regression from Scratch ===</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
    
    <span class="c1"># Method 1: Gradient Descent
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"1. Training with Gradient Descent:"</span><span class="p">)</span>
    <span class="n">model_gd</span> <span class="o">=</span> <span class="n">LinearRegressionFromScratch</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">model_gd</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'gradient_descent'</span><span class="p">)</span>
    
    <span class="n">train_r2_gd</span> <span class="o">=</span> <span class="n">model_gd</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">test_r2_gd</span> <span class="o">=</span> <span class="n">model_gd</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"   Training R¬≤: </span><span class="si">{</span><span class="n">train_r2_gd</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"   Test R¬≤: </span><span class="si">{</span><span class="n">test_r2_gd</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    
    <span class="c1"># Method 2: Normal Equation
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">2. Training with Normal Equation:"</span><span class="p">)</span>
    <span class="n">model_ne</span> <span class="o">=</span> <span class="n">LinearRegressionFromScratch</span><span class="p">()</span>
    <span class="n">model_ne</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'normal_equation'</span><span class="p">)</span>
    
    <span class="n">train_r2_ne</span> <span class="o">=</span> <span class="n">model_ne</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">test_r2_ne</span> <span class="o">=</span> <span class="n">model_ne</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"   Training R¬≤: </span><span class="si">{</span><span class="n">train_r2_ne</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"   Test R¬≤: </span><span class="si">{</span><span class="n">test_r2_ne</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    
    <span class="c1"># Compare parameters
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">3. Parameter Comparison:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"   Gradient Descent - Weight: </span><span class="si">{</span><span class="n">model_gd</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Bias: </span><span class="si">{</span><span class="n">model_gd</span><span class="p">.</span><span class="n">bias</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"   Normal Equation - Weight: </span><span class="si">{</span><span class="n">model_ne</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Bias: </span><span class="si">{</span><span class="n">model_ne</span><span class="p">.</span><span class="n">bias</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    
    <span class="c1"># Visualization
</span>    <span class="n">model_gd</span><span class="p">.</span><span class="n">plot_training_progress</span><span class="p">()</span>
    <span class="n">model_gd</span><span class="p">.</span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model_gd</span><span class="p">,</span> <span class="n">model_ne</span>

<span class="c1"># Run the demonstration
</span><span class="n">model_gd</span><span class="p">,</span> <span class="n">model_ne</span> <span class="o">=</span> <span class="n">demonstrate_linear_regression</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<h2 id="-curated-resources">üîó Curated Resources</h2>

<h3 id="essential-reading">Essential Reading</h3>
<ul>
  <li><a href="https://aima.cs.berkeley.edu/">AIMA Chapter 18</a> - Learning from Examples</li>
  <li><a href="https://www.statlearning.com/">Introduction to Statistical Learning</a> - Free comprehensive textbook</li>
  <li><a href="https://hastie.su.domains/ElemStatLearn/">Elements of Statistical Learning</a> - Advanced mathematical treatment</li>
</ul>

<h3 id="video-resources">Video Resources</h3>
<ul>
  <li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning Course - Andrew Ng</a> - Classic introduction</li>
  <li><a href="https://www.youtube.com/watch?v=aircAruvnKk">ML Fundamentals - 3Blue1Brown</a> - Neural networks from first principles</li>
  <li><a href="https://www.youtube.com/watch?v=sDv4f4s2SB8">Gradient Descent - Josh Starmer</a> - StatQuest explanation</li>
</ul>

<h3 id="interactive-learning">Interactive Learning</h3>
<ul>
  <li><a href="https://ml-playground.com/">ML Playground</a> - Visualize algorithms</li>
  <li><a href="https://seeing-theory.brown.edu/">Seeing Theory</a> - Interactive statistics</li>
  <li><a href="https://ai.google/education/">Google‚Äôs AI Education</a> - Courses and tools</li>
</ul>

<h3 id="practice-problems">Practice Problems</h3>
<ul>
  <li><a href="https://www.kaggle.com/learn">Kaggle Learn</a> - Free micro-courses</li>
  <li><a href="https://machinelearningmastery.com/">Machine Learning Mastery</a> - Practical tutorials</li>
  <li><a href="https://paperswithcode.com/">Papers with Code</a> - Latest research implementations</li>
</ul>

<hr />

<h2 id="-key-concepts-summary">üéØ Key Concepts Summary</h2>

<h3 id="core-principles">Core Principles</h3>
<ul>
  <li><strong>Learning from data</strong> instead of explicit programming</li>
  <li><strong>Generalization</strong> to unseen examples is the goal</li>
  <li><strong>Different types</strong> for different problem structures</li>
</ul>

<h3 id="critical-concepts">Critical Concepts</h3>
<ul>
  <li><strong>Bias-variance tradeoff</strong> affects model performance</li>
  <li><strong>Training/validation/test splits</strong> prevent overfitting</li>
  <li><strong>Cross-validation</strong> for robust model evaluation</li>
  <li><strong>Gradient descent</strong> as universal optimization tool</li>
</ul>

<h3 id="practical-workflow">Practical Workflow</h3>
<ol>
  <li><strong>Problem formulation</strong> and success metrics</li>
  <li><strong>Data collection</strong> and exploratory analysis</li>
  <li><strong>Preprocessing</strong> and feature engineering</li>
  <li><strong>Model selection</strong> and hyperparameter tuning</li>
  <li><strong>Evaluation</strong> and performance analysis</li>
  <li><strong>Deployment</strong> and ongoing monitoring</li>
</ol>

<hr />

<h2 id="-discussion-questions">ü§î Discussion Questions</h2>

<ol>
  <li>When would you choose supervised vs. unsupervised learning?</li>
  <li>How do you know if you have enough training data?</li>
  <li>What are the ethical implications of algorithmic decision-making?</li>
  <li>How does machine learning relate to traditional statistical methods?</li>
  <li>What role does domain expertise play in successful ML projects?</li>
</ol>

<hr />

<h2 id="-looking-ahead">üîç Looking Ahead</h2>

<p>Next week, we‚Äôll dive deeper into <strong>supervised learning algorithms</strong>, starting with linear and logistic regression, then exploring k-nearest neighbors and decision trees. We‚Äôll implement these algorithms and apply them to real datasets!</p>

<p><strong>Preview of Week 8:</strong></p>
<ul>
  <li>Linear regression and regularization techniques</li>
  <li>Logistic regression for classification</li>
  <li>k-Nearest Neighbors algorithm</li>
  <li>Decision trees and feature importance</li>
</ul>

<hr />

<h3 id="navigation-1">Navigation</h3>
<p><a href="index.html">üè† Home</a> | <a href="week06.html">‚Üê Previous: Week 6</a> | <a href="week08.html">Next: Week 8 ‚Üí</a></p>

<hr />

<p><em>‚ÄúMachine learning is not about the algorithm, it‚Äôs about understanding your data and solving real problems.‚Äù</em></p>

</div>

<style>
.week-header {
  margin: -2rem -1.5rem 3rem -1.5rem;
  padding: 2rem 1.5rem;
  background: linear-gradient(135deg, rgba(99, 102, 241, 0.08) 0%, rgba(139, 92, 246, 0.08) 100%);
  border-radius: 0 0 1.5rem 1.5rem;
}

.week-hero {
  display: flex;
  align-items: center;
  gap: 2rem;
  max-width: var(--container-max-width);
  margin: 0 auto;
}

.week-number-large {
  font-size: 4rem;
  font-weight: 800;
  background: var(--primary-gradient);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
  opacity: 0.8;
  min-width: 5rem;
  text-align: center;
}

.week-title-section {
  flex: 1;
}

.week-title-section h1 {
  margin-bottom: 1rem;
  font-size: 2rem;
}

.week-progress-bar {
  width: 100%;
  height: 8px;
  background: rgba(255, 255, 255, 0.3);
  border-radius: 4px;
  overflow: hidden;
}

.week-progress-bar .progress-fill {
  height: 100%;
  background: var(--primary-gradient);
  border-radius: 4px;
  transition: width 0.8s ease;
}

.week-navigation {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-top: 2rem;
  max-width: var(--container-max-width);
  margin-left: auto;
  margin-right: auto;
}

.week-content {
  animation: fadeInUp 0.6s ease-out;
}

.learning-objectives {
  background: linear-gradient(135deg, rgba(99, 102, 241, 0.05), rgba(139, 92, 246, 0.05));
  border: 1px solid rgba(99, 102, 241, 0.1);
  border-left: 4px solid var(--primary-color);
  padding: 2rem;
  border-radius: var(--border-radius);
  margin: 2rem 0;
  box-shadow: var(--shadow);
}

.learning-objectives h2 {
  color: var(--primary-color);
  margin-bottom: 1.5rem;
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

.learning-objectives ul {
  list-style: none;
  padding: 0;
}

.learning-objectives li {
  position: relative;
  padding-left: 2rem;
  margin-bottom: 1rem;
  color: var(--text-secondary);
}

.learning-objectives li::before {
  content: '‚úì';
  position: absolute;
  left: 0;
  color: var(--accent-3);
  font-weight: bold;
  font-size: 1.2rem;
}

@media (max-width: 768px) {
  .week-header {
    margin: -2rem -1rem 2rem -1rem;
    padding: 1.5rem 1rem;
  }
  
  .week-hero {
    flex-direction: column;
    text-align: center;
    gap: 1rem;
  }
  
  .week-number-large {
    font-size: 3rem;
  }
  
  .week-title-section h1 {
    font-size: 1.5rem;
  }
  
  .week-navigation {
    flex-direction: column;
    gap: 1rem;
  }
}
</style>
      </div>
    </main>

    <!-- Footer -->
    <footer class="site-footer">
      <div class="footer-container">
        <div class="footer-content">
          <div class="footer-section">
            <h3>AI Learning Students</h3>
            <p>A comprehensive educational resource for learning Artificial Intelligence - structured 14-week curriculum for undergraduate students</p>
          </div>
          <div class="footer-section">
            <h4>Quick Links</h4>
            <ul>
              <li><a href="/ai-learning-students/">Course Overview</a></li>
              <li><a href="https://github.com/irfan-sec/ai-learning-students">GitHub Repository</a></li>
              <li><a href="/ai-learning-students/week01.html">Get Started</a></li>
            </ul>
          </div>
          <div class="footer-section">
            <h4>Resources</h4>
            <ul>
              <li><a href="https://aima.cs.berkeley.edu/">AIMA Textbook</a></li>
              <li><a href="https://scikit-learn.org/">Scikit-learn</a></li>
              <li><a href="https://www.fast.ai/">Fast.ai</a></li>
            </ul>
          </div>
        </div>
        <div class="footer-bottom">
          <p>&copy; 2024 AI Learning Community. Licensed under MIT License.</p>
        </div>
      </div>
    </footer>
  </div>

  <!-- Scripts -->
  <script src="/ai-learning-students/assets/js/theme.js"></script>
</body>
</html>