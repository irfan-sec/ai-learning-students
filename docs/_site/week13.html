<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="A comprehensive educational resource for learning Artificial Intelligence - structured 14-week curriculum for undergraduate students">
  
  <title>Week 13: Ethics in AI | AI Learning Students</title>
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <!-- Custom styles -->
  <link rel="stylesheet" href="/ai-learning-students/assets/css/style.css">
</head>
<body>
  <div class="page-wrapper">
    <!-- Navigation Header -->
    <header class="site-header">
      <nav class="nav-container">
        <div class="nav-brand">
          <a href="/ai-learning-students/" class="brand-link">
            <div class="brand-icon">
              <i class="fas fa-brain"></i>
            </div>
            <span class="brand-text">AI Learning Students</span>
          </a>
        </div>
        
        <div class="nav-menu" id="navMenu">
          <div class="nav-links">
            <a href="/ai-learning-students/" class="nav-link ">
              <i class="fas fa-home"></i>
              Home
            </a>
            <div class="nav-dropdown">
              <a href="#" class="nav-link dropdown-toggle">
                <i class="fas fa-book"></i>
                Weeks
                <i class="fas fa-chevron-down"></i>
              </a>
              <div class="dropdown-menu">
                
                  
                  <a href="/ai-learning-students/week01.html" class="dropdown-link">
                    Week 1
                  </a>
                
                  
                  <a href="/ai-learning-students/week02.html" class="dropdown-link">
                    Week 2
                  </a>
                
                  
                  <a href="/ai-learning-students/week03.html" class="dropdown-link">
                    Week 3
                  </a>
                
                  
                  <a href="/ai-learning-students/week04.html" class="dropdown-link">
                    Week 4
                  </a>
                
                  
                  <a href="/ai-learning-students/week05.html" class="dropdown-link">
                    Week 5
                  </a>
                
                  
                  <a href="/ai-learning-students/week06.html" class="dropdown-link">
                    Week 6
                  </a>
                
                  
                  <a href="/ai-learning-students/week07.html" class="dropdown-link">
                    Week 7
                  </a>
                
                  
                  <a href="/ai-learning-students/week08.html" class="dropdown-link">
                    Week 8
                  </a>
                
                  
                  <a href="/ai-learning-students/week09.html" class="dropdown-link">
                    Week 9
                  </a>
                
                  
                  <a href="/ai-learning-students/week10.html" class="dropdown-link">
                    Week 10
                  </a>
                
                  
                  <a href="/ai-learning-students/week11.html" class="dropdown-link">
                    Week 11
                  </a>
                
                  
                  <a href="/ai-learning-students/week12.html" class="dropdown-link">
                    Week 12
                  </a>
                
                  
                  <a href="/ai-learning-students/week13.html" class="dropdown-link">
                    Week 13
                  </a>
                
                  
                  <a href="/ai-learning-students/week14.html" class="dropdown-link">
                    Week 14
                  </a>
                
              </div>
            </div>
          </div>
          
          <div class="nav-progress">
            <div class="progress-indicator">
              <div class="progress-circle">
                <div class="progress-fill"></div>
                <span class="progress-text">0%</span>
              </div>
            </div>
          </div>
        </div>
        
        <button class="mobile-menu-toggle" id="mobileToggle">
          <span></span>
          <span></span>
          <span></span>
        </button>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main-content">
      <div class="content-container">
        <div class="week-header">
  <div class="week-hero">
    <div class="week-number-large"></div>
    <div class="week-title-section">
      <h1>Week 13: Ethics in AI</h1>
      <div class="week-progress-bar">
        <div class="progress-fill" style="width: 0.0%"></div>
      </div>
    </div>
  </div>
  
  <div class="week-navigation">
    
    
    
    <a href="index.html" class="nav-button home">
      <i class="fas fa-home"></i>
      Course Overview
    </a>
    
    
      <a href="week01.html" class="nav-button next">
        Next Week
        <i class="fas fa-chevron-right"></i>
      </a>
    
  </div>
</div>

<div class="week-content">
  <h3 id="navigation">Navigation</h3>
<p><a href="index.html">üè† Home</a> | <a href="week12.html">‚Üê Previous: Week 12</a> | <a href="week14.html">Next: Week 14 ‚Üí</a></p>

<hr />

<h1 id="week-13-ethics-in-ai">Week 13: Ethics in AI</h1>

<h2 id="-learning-objectives">üìö Learning Objectives</h2>
<p>By the end of this week, students will be able to:</p>
<ul>
  <li>Understand the ethical implications of AI systems in society</li>
  <li>Identify sources of bias in AI algorithms and data</li>
  <li>Explain concepts of fairness, accountability, and transparency in AI</li>
  <li>Analyze real-world case studies of AI ethical dilemmas</li>
  <li>Discuss approaches to responsible AI development and deployment</li>
</ul>

<hr />

<h2 id="Ô∏è-why-ai-ethics-matters">‚öñÔ∏è Why AI Ethics Matters</h2>

<p>As AI systems become increasingly prevalent in society, they impact human lives in profound ways. From hiring decisions to criminal justice, from healthcare to financial services, AI is no longer just a technical challenge‚Äîit‚Äôs a societal responsibility.</p>

<p><img src="images/ai-ethics-overview.png" alt="AI Ethics Overview" /></p>

<h3 id="the-stakes-are-high">The Stakes Are High</h3>
<ul>
  <li><strong>Individual Impact:</strong> AI decisions affect careers, health, freedom, and opportunities</li>
  <li><strong>Societal Impact:</strong> AI can perpetuate or amplify existing inequalities</li>
  <li><strong>Global Impact:</strong> AI systems shape information, democracy, and economic structures</li>
  <li><strong>Future Impact:</strong> Today‚Äôs AI decisions set precedents for tomorrow‚Äôs more powerful systems</li>
</ul>

<h3 id="why-this-matters-to-you">Why This Matters to You</h3>
<p>As future AI practitioners, you will be responsible for building systems that affect millions of people. Understanding these ethical challenges isn‚Äôt just academic‚Äîit‚Äôs essential for professional practice.</p>

<hr />

<h2 id="-core-ethical-principles-in-ai">üîç Core Ethical Principles in AI</h2>

<h3 id="1-fairness-and-non-discrimination">1. <strong>Fairness and Non-Discrimination</strong></h3>
<p>AI systems should not unfairly discriminate against individuals or groups.</p>

<p><strong>Key Questions:</strong></p>
<ul>
  <li>Does the system treat all groups equitably?</li>
  <li>Are outcomes distributed fairly across demographics?</li>
  <li>Does the system perpetuate historical biases?</li>
</ul>

<p><strong>Example:</strong> A hiring algorithm that systematically rejects qualified women candidates because it was trained on historical data from male-dominated industries.</p>

<h3 id="2-transparency-and-explainability">2. <strong>Transparency and Explainability</strong></h3>
<p>People should understand how AI systems make decisions that affect them.</p>

<p><strong>Levels of Transparency:</strong></p>
<ul>
  <li><strong>Algorithmic:</strong> How does the system work internally?</li>
  <li><strong>Procedural:</strong> What process led to this decision?</li>
  <li><strong>Outcome:</strong> Why was this particular decision made?</li>
</ul>

<p><strong>Challenge:</strong> Complex models (deep learning) often lack interpretability.</p>

<h3 id="3-accountability-and-responsibility">3. <strong>Accountability and Responsibility</strong></h3>
<p>There must be clear lines of responsibility for AI system outcomes.</p>

<p><strong>Key Questions:</strong></p>
<ul>
  <li>Who is responsible when AI makes a mistake?</li>
  <li>How do we ensure proper oversight?</li>
  <li>What recourse do affected individuals have?</li>
</ul>

<h3 id="4-privacy-and-data-protection">4. <strong>Privacy and Data Protection</strong></h3>
<p>AI systems must respect individual privacy and data rights.</p>

<p><strong>Privacy Concerns:</strong></p>
<ul>
  <li><strong>Data collection:</strong> What data is gathered and how?</li>
  <li><strong>Data use:</strong> How is personal data used in training and inference?</li>
  <li><strong>Data sharing:</strong> Who has access to personal information?</li>
  <li><strong>Re-identification:</strong> Can anonymized data be traced back to individuals?</li>
</ul>

<h3 id="5-human-autonomy-and-agency">5. <strong>Human Autonomy and Agency</strong></h3>
<p>AI should augment rather than replace human decision-making in critical areas.</p>

<p><strong>Considerations:</strong></p>
<ul>
  <li>Maintaining human oversight in important decisions</li>
  <li>Preserving human skills and capabilities</li>
  <li>Ensuring humans can override AI recommendations</li>
</ul>

<hr />

<h2 id="-sources-of-bias-in-ai-systems">üìä Sources of Bias in AI Systems</h2>

<h3 id="1-historical-bias-in-training-data">1. <strong>Historical Bias in Training Data</strong></h3>
<p>AI systems learn from historical data, which often reflects past discrimination.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Biased hiring data
</span><span class="n">historical_hires</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'software_engineer'</span><span class="p">:</span> <span class="p">{</span><span class="s">'male'</span><span class="p">:</span> <span class="mi">90</span><span class="p">,</span> <span class="s">'female'</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span>  <span class="c1"># Historical bias
</span>    <span class="s">'nurse'</span><span class="p">:</span> <span class="p">{</span><span class="s">'male'</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="s">'female'</span><span class="p">:</span> <span class="mi">85</span><span class="p">},</span>
    <span class="s">'teacher'</span><span class="p">:</span> <span class="p">{</span><span class="s">'male'</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span> <span class="s">'female'</span><span class="p">:</span> <span class="mi">75</span><span class="p">}</span>
<span class="p">}</span>

<span class="c1"># Problem: AI trained on this data will perpetuate these patterns
</span><span class="k">def</span> <span class="nf">analyze_hiring_bias</span><span class="p">(</span><span class="n">job_data</span><span class="p">):</span>
    <span class="s">"""Detect potential gender bias in hiring data"""</span>
    <span class="k">for</span> <span class="n">job</span><span class="p">,</span> <span class="n">demographics</span> <span class="ow">in</span> <span class="n">job_data</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">total</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">demographics</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">male_ratio</span> <span class="o">=</span> <span class="n">demographics</span><span class="p">[</span><span class="s">'male'</span><span class="p">]</span> <span class="o">/</span> <span class="n">total</span>
        <span class="n">female_ratio</span> <span class="o">=</span> <span class="n">demographics</span><span class="p">[</span><span class="s">'female'</span><span class="p">]</span> <span class="o">/</span> <span class="n">total</span>
        
        <span class="c1"># Flag potential bias (threshold: 70-30 split)
</span>        <span class="k">if</span> <span class="n">male_ratio</span> <span class="o">&gt;</span> <span class="mf">0.7</span> <span class="ow">or</span> <span class="n">female_ratio</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"‚ö†Ô∏è Potential bias in </span><span class="si">{</span><span class="n">job</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">male_ratio</span><span class="p">:.</span><span class="mi">0</span><span class="o">%</span><span class="si">}</span><span class="s"> male, </span><span class="si">{</span><span class="n">female_ratio</span><span class="p">:.</span><span class="mi">0</span><span class="o">%</span><span class="si">}</span><span class="s"> female"</span><span class="p">)</span>

<span class="n">analyze_hiring_bias</span><span class="p">(</span><span class="n">historical_hires</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="2-representation-bias">2. <strong>Representation Bias</strong></h3>
<p>Training data may not represent all groups equally.</p>

<p><strong>Examples:</strong></p>
<ul>
  <li><strong>Facial recognition:</strong> Poor performance on darker skin tones</li>
  <li><strong>Voice recognition:</strong> Biased toward certain accents or dialects</li>
  <li><strong>Medical AI:</strong> Trained primarily on data from one demographic group</li>
</ul>

<h3 id="3-measurement-bias">3. <strong>Measurement Bias</strong></h3>
<p>Different groups may be measured or evaluated differently.</p>

<p><strong>Examples:</strong></p>
<ul>
  <li><strong>Standardized tests:</strong> May not measure intelligence equally across cultures</li>
  <li><strong>Performance reviews:</strong> Subjective evaluations may contain unconscious bias</li>
  <li><strong>Health indicators:</strong> May have different meanings across populations</li>
</ul>

<h3 id="4-evaluation-bias">4. <strong>Evaluation Bias</strong></h3>
<p>Using inappropriate benchmarks or metrics for different groups.</p>

<p><strong>Example:</strong> A model optimized for overall accuracy might perform poorly on minority groups if they‚Äôre underrepresented in the dataset.</p>

<hr />

<h2 id="-algorithmic-fairness">üéØ Algorithmic Fairness</h2>

<h3 id="defining-fairness-mathematically">Defining Fairness Mathematically</h3>

<p><strong>Statistical Parity (Demographic Parity)</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>P(≈∂ = 1 | A = 0) = P(≈∂ = 1 | A = 1)
</code></pre></div></div>
<p>Equal probability of positive outcome across groups.</p>

<p><strong>Equal Opportunity</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>P(≈∂ = 1 | A = 0, Y = 1) = P(≈∂ = 1 | A = 1, Y = 1)
</code></pre></div></div>
<p>Equal true positive rates across groups.</p>

<p><strong>Equalized Odds</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>P(≈∂ = 1 | A = 0, Y = y) = P(≈∂ = 1 | A = 1, Y = y) for y ‚àà {0,1}
</code></pre></div></div>
<p>Equal true positive and false positive rates across groups.</p>

<h3 id="the-impossibility-of-fairness">The Impossibility of Fairness</h3>
<p><strong>Important:</strong> These fairness criteria often conflict with each other! Perfect fairness across all metrics simultaneously is often mathematically impossible.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="k">def</span> <span class="nf">calculate_fairness_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sensitive_attr</span><span class="p">):</span>
    <span class="s">"""Calculate various fairness metrics"""</span>
    <span class="c1"># Split by sensitive attribute
</span>    <span class="n">group_0</span> <span class="o">=</span> <span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">group_1</span> <span class="o">=</span> <span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">1</span>
    
    <span class="c1"># Statistical Parity
</span>    <span class="n">positive_rate_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">group_0</span><span class="p">])</span>
    <span class="n">positive_rate_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">group_1</span><span class="p">])</span>
    <span class="n">statistical_parity</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">positive_rate_0</span> <span class="o">-</span> <span class="n">positive_rate_1</span><span class="p">)</span>
    
    <span class="c1"># Equal Opportunity (True Positive Rate equality)
</span>    <span class="n">tp_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span><span class="p">[</span><span class="n">group_0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">group_0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">fn_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span><span class="p">[</span><span class="n">group_0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">group_0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">tpr_0</span> <span class="o">=</span> <span class="n">tp_0</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp_0</span> <span class="o">+</span> <span class="n">fn_0</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">tp_0</span> <span class="o">+</span> <span class="n">fn_0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
    
    <span class="n">tp_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span><span class="p">[</span><span class="n">group_1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">group_1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">fn_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span><span class="p">[</span><span class="n">group_1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">group_1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">tpr_1</span> <span class="o">=</span> <span class="n">tp_1</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp_1</span> <span class="o">+</span> <span class="n">fn_1</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">tp_1</span> <span class="o">+</span> <span class="n">fn_1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
    
    <span class="n">equal_opportunity</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">tpr_0</span> <span class="o">-</span> <span class="n">tpr_1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s">'statistical_parity_difference'</span><span class="p">:</span> <span class="n">statistical_parity</span><span class="p">,</span>
        <span class="s">'equal_opportunity_difference'</span><span class="p">:</span> <span class="n">equal_opportunity</span><span class="p">,</span>
        <span class="s">'tpr_group_0'</span><span class="p">:</span> <span class="n">tpr_0</span><span class="p">,</span>
        <span class="s">'tpr_group_1'</span><span class="p">:</span> <span class="n">tpr_1</span>
    <span class="p">}</span>

<span class="c1"># Example usage
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Biased predictions
</span><span class="n">sensitive_attr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="n">calculate_fairness_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sensitive_attr</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Fairness Metrics:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="p">:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="-real-world-case-studies">üì± Real-World Case Studies</h2>

<h3 id="case-study-1-amazons-hiring-algorithm-2018">Case Study 1: Amazon‚Äôs Hiring Algorithm (2018)</h3>

<p><strong>Background:</strong> Amazon developed an AI system to rank job candidates‚Äô resumes.</p>

<p><strong>The Problem:</strong></p>
<ul>
  <li>System was trained on 10 years of historical hiring data</li>
  <li>Historical data reflected male-dominated tech industry</li>
  <li>Algorithm learned to penalize resumes mentioning ‚Äúwomen‚Äù (e.g., ‚Äúwomen‚Äôs chess club captain‚Äù)</li>
</ul>

<p><strong>Ethical Issues:</strong></p>
<ul>
  <li><strong>Bias amplification:</strong> System amplified existing gender bias</li>
  <li><strong>Lack of diversity:</strong> Reinforced homogeneous hiring patterns</li>
  <li><strong>Transparency:</strong> Candidates unaware of algorithmic evaluation</li>
</ul>

<p><strong>Outcome:</strong> Amazon scrapped the system after discovering the bias.</p>

<p><strong>Lessons:</strong></p>
<ul>
  <li>Historical data can perpetuate discrimination</li>
  <li>Bias testing is crucial before deployment</li>
  <li>Diverse development teams help identify problems</li>
</ul>

<h3 id="case-study-2-compas-recidivism-prediction">Case Study 2: COMPAS Recidivism Prediction</h3>

<p><strong>Background:</strong> COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) predicts likelihood of reoffending.</p>

<p><strong>The Controversy:</strong></p>
<ul>
  <li>System showed higher false positive rates for Black defendants</li>
  <li>Black defendants more likely to be incorrectly labeled high-risk</li>
  <li>White defendants more likely to be incorrectly labeled low-risk</li>
</ul>

<p><strong>Ethical Dilemma:</strong></p>
<ul>
  <li><strong>Predictive accuracy:</strong> System was statistically accurate overall</li>
  <li><strong>Disparate impact:</strong> But outcomes differed significantly by race</li>
  <li><strong>Lack of transparency:</strong> Algorithm details were proprietary</li>
</ul>

<p><strong>Ongoing Debate:</strong></p>
<ul>
  <li>Should we use accurate but biased systems?</li>
  <li>How do we balance efficiency with fairness?</li>
  <li>What level of transparency is required for criminal justice AI?</li>
</ul>

<h3 id="case-study-3-facial-recognition-and-surveillance">Case Study 3: Facial Recognition and Surveillance</h3>

<p><strong>Background:</strong> Widespread deployment of facial recognition technology.</p>

<p><strong>Ethical Concerns:</strong></p>

<p><strong>Bias Issues:</strong></p>
<ul>
  <li>Higher error rates for women and people of color</li>
  <li>Led to false arrests and wrongful detentions</li>
</ul>

<p><strong>Privacy Concerns:</strong></p>
<ul>
  <li>Constant surveillance without consent</li>
  <li>Potential for government overreach</li>
  <li>Data security and misuse risks</li>
</ul>

<p><strong>Societal Impact:</strong></p>
<ul>
  <li>Chilling effect on freedom of assembly</li>
  <li>Disproportionate impact on marginalized communities</li>
  <li>Normalization of surveillance</li>
</ul>

<p><strong>Responses:</strong></p>
<ul>
  <li>Some cities banned facial recognition by government agencies</li>
  <li>Companies restricted sale of facial recognition technology</li>
  <li>Calls for regulation and oversight</li>
</ul>

<hr />

<h2 id="Ô∏è-approaches-to-responsible-ai">üõ°Ô∏è Approaches to Responsible AI</h2>

<h3 id="1-diverse-and-inclusive-development-teams">1. <strong>Diverse and Inclusive Development Teams</strong></h3>

<p><strong>Why It Matters:</strong></p>
<ul>
  <li>Different perspectives identify different problems</li>
  <li>Inclusive teams build more inclusive systems</li>
  <li>Representation matters at all levels</li>
</ul>

<p><strong>Best Practices:</strong></p>
<ul>
  <li>Diverse hiring in AI roles</li>
  <li>Include social scientists, ethicists, domain experts</li>
  <li>Community engagement and stakeholder input</li>
</ul>

<h3 id="2-bias-detection-and-mitigation">2. <strong>Bias Detection and Mitigation</strong></h3>

<p><strong>Pre-processing Approaches:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>

<span class="k">def</span> <span class="nf">preprocess_for_fairness</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sensitive_feature</span><span class="p">):</span>
    <span class="s">"""Pre-processing to reduce bias"""</span>
    
    <span class="c1"># Remove sensitive features from training
</span>    <span class="n">X_processed</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">sensitive_feature</span><span class="p">])</span>
    
    <span class="c1"># Balance dataset across sensitive groups
</span>    <span class="n">smote</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">X_balanced</span><span class="p">,</span> <span class="n">y_balanced</span> <span class="o">=</span> <span class="n">smote</span><span class="p">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_processed</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Standardize features
</span>    <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_balanced</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X_scaled</span><span class="p">,</span> <span class="n">y_balanced</span><span class="p">,</span> <span class="n">scaler</span>
</code></pre></div></div>

<p><strong>In-processing Approaches:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fairness_regularized_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sensitive_attr</span><span class="p">,</span> <span class="n">lambda_fair</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="s">"""Loss function with fairness regularization"""</span>
    
    <span class="c1"># Standard prediction loss (e.g., cross-entropy)
</span>    <span class="n">prediction_loss</span> <span class="o">=</span> <span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    
    <span class="c1"># Fairness penalty - discourage disparate impact
</span>    <span class="n">group_0_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">group_1_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="n">fairness_penalty</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">group_0_pred</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">group_1_pred</span><span class="p">))</span>
    
    <span class="c1"># Combined loss
</span>    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">prediction_loss</span> <span class="o">+</span> <span class="n">lambda_fair</span> <span class="o">*</span> <span class="n">fairness_penalty</span>
    
    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></div>

<p><strong>Post-processing Approaches:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calibrate_for_fairness</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">sensitive_attr</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'equal_opportunity'</span><span class="p">):</span>
    <span class="s">"""Adjust predictions to achieve fairness"""</span>
    
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s">'equal_opportunity'</span><span class="p">:</span>
        <span class="c1"># Adjust thresholds to equalize true positive rates
</span>        <span class="n">group_0_threshold</span> <span class="o">=</span> <span class="n">find_threshold_for_tpr</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">target_tpr</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
        <span class="n">group_1_threshold</span> <span class="o">=</span> <span class="n">find_threshold_for_tpr</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">target_tpr</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
        
        <span class="c1"># Apply different thresholds
</span>        <span class="n">calibrated_predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">calibrated_predictions</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">group_0_threshold</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">calibrated_predictions</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">sensitive_attr</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">group_1_threshold</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">calibrated_predictions</span>
</code></pre></div></div>

<h3 id="3-explainable-ai-xai">3. <strong>Explainable AI (XAI)</strong></h3>

<p><strong>Techniques:</strong></p>
<ul>
  <li><strong>Local explanations:</strong> Why this particular decision?</li>
  <li><strong>Global explanations:</strong> How does the system work overall?</li>
  <li><strong>Feature importance:</strong> Which factors mattered most?</li>
  <li><strong>Counterfactual explanations:</strong> What would change the outcome?</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">shap</span>
<span class="kn">from</span> <span class="nn">lime</span> <span class="kn">import</span> <span class="n">lime_text</span>

<span class="k">def</span> <span class="nf">explain_model_decision</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">instance_idx</span><span class="p">):</span>
    <span class="s">"""Provide multiple explanations for a model decision"""</span>
    
    <span class="c1"># SHAP explanation (global and local)
</span>    <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="p">.</span><span class="n">Explainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># Feature importance for specific instance
</span>    <span class="n">instance_explanation</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">instance_idx</span><span class="p">]</span>
    
    <span class="c1"># LIME explanation for comparison
</span>    <span class="n">lime_explainer</span> <span class="o">=</span> <span class="n">lime_text</span><span class="p">.</span><span class="n">LimeTextExplainer</span><span class="p">()</span>
    <span class="n">lime_explanation</span> <span class="o">=</span> <span class="n">lime_explainer</span><span class="p">.</span><span class="n">explain_instance</span><span class="p">(</span>
        <span class="n">X_test</span><span class="p">[</span><span class="n">instance_idx</span><span class="p">],</span> 
        <span class="n">model</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">,</span>
        <span class="n">num_features</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s">'shap_values'</span><span class="p">:</span> <span class="n">instance_explanation</span><span class="p">,</span>
        <span class="s">'lime_explanation'</span><span class="p">:</span> <span class="n">lime_explanation</span><span class="p">,</span>
        <span class="s">'feature_importance'</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">shap_values</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">instance_idx</span><span class="p">]))</span>
    <span class="p">}</span>
</code></pre></div></div>

<h3 id="4-ai-governance-and-oversight">4. <strong>AI Governance and Oversight</strong></h3>

<p><strong>Internal Governance:</strong></p>
<ul>
  <li>Ethics review boards</li>
  <li>Regular bias audits</li>
  <li>Impact assessments</li>
  <li>Clear policies and procedures</li>
</ul>

<p><strong>External Oversight:</strong></p>
<ul>
  <li>Regulatory frameworks</li>
  <li>Industry standards</li>
  <li>Third-party audits</li>
  <li>Public accountability</li>
</ul>

<h3 id="5-human-in-the-loop-systems">5. <strong>Human-in-the-Loop Systems</strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HumanInTheLoopClassifier</span><span class="p">:</span>
    <span class="s">"""AI system with human oversight"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ai_model</span><span class="p">,</span> <span class="n">confidence_threshold</span><span class="o">=</span><span class="mf">0.8</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ai_model</span> <span class="o">=</span> <span class="n">ai_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">confidence_threshold</span> <span class="o">=</span> <span class="n">confidence_threshold</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">human_review_queue</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""Make predictions with human fallback"""</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ai_model</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">confident_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pred_proba</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
            <span class="n">max_confidence</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">pred_proba</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">max_confidence</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">confidence_threshold</span><span class="p">:</span>
                <span class="c1"># AI is confident - use AI prediction
</span>                <span class="n">confident_predictions</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_proba</span><span class="p">.</span><span class="n">argmax</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Low confidence - queue for human review
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">human_review_queue</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
                    <span class="s">'instance_id'</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                    <span class="s">'features'</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="s">'ai_prediction'</span><span class="p">:</span> <span class="n">pred_proba</span><span class="p">.</span><span class="n">argmax</span><span class="p">(),</span>
                    <span class="s">'confidence'</span><span class="p">:</span> <span class="n">max_confidence</span>
                <span class="p">})</span>
                <span class="n">confident_predictions</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># Pending human review
</span>        
        <span class="k">return</span> <span class="n">confident_predictions</span>
    
    <span class="k">def</span> <span class="nf">get_human_review_queue</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""Get cases that need human review"""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">human_review_queue</span>
</code></pre></div></div>

<hr />

<h2 id="-hands-on-exercise-bias-detection-tool">üíª Hands-On Exercise: Bias Detection Tool</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="k">class</span> <span class="nc">BiasDetectionTool</span><span class="p">:</span>
    <span class="s">"""Comprehensive bias detection and analysis tool"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sensitive_attributes</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sensitive_attributes</span> <span class="o">=</span> <span class="n">sensitive_attributes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias_report</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">def</span> <span class="nf">analyze_dataset_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Analyze bias in the dataset"""</span>
        <span class="n">dataset_bias</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">sensitive_attributes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
                <span class="c1"># Distribution analysis
</span>                <span class="n">attr_distribution</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">attr</span><span class="p">].</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                <span class="n">outcome_by_attr</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">attr</span><span class="p">)[</span><span class="n">y</span><span class="p">.</span><span class="n">name</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
                
                <span class="n">dataset_bias</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s">'distribution'</span><span class="p">:</span> <span class="n">attr_distribution</span><span class="p">.</span><span class="n">to_dict</span><span class="p">(),</span>
                    <span class="s">'outcome_rates'</span><span class="p">:</span> <span class="n">outcome_by_attr</span><span class="p">.</span><span class="n">to_dict</span><span class="p">()</span>
                <span class="p">}</span>
        
        <span class="k">return</span> <span class="n">dataset_bias</span>
    
    <span class="k">def</span> <span class="nf">analyze_model_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
        <span class="s">"""Analyze bias in model predictions"""</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">pred_proba</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
        
        <span class="n">model_bias</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">sensitive_attributes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
                <span class="c1"># Group-specific performance
</span>                <span class="n">groups</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">attr</span><span class="p">].</span><span class="n">unique</span><span class="p">()</span>
                <span class="n">group_metrics</span> <span class="o">=</span> <span class="p">{}</span>
                
                <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">:</span>
                    <span class="n">group_mask</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">==</span> <span class="n">group</span>
                    <span class="n">group_y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">group_mask</span><span class="p">]</span>
                    <span class="n">group_y_pred</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">group_mask</span><span class="p">]</span>
                    
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">group_y_true</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># Calculate metrics for this group
</span>                        <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>
                        
                        <span class="n">group_metrics</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                            <span class="s">'accuracy'</span><span class="p">:</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">group_y_true</span><span class="p">,</span> <span class="n">group_y_pred</span><span class="p">),</span>
                            <span class="s">'precision'</span><span class="p">:</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">group_y_true</span><span class="p">,</span> <span class="n">group_y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'weighted'</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                            <span class="s">'recall'</span><span class="p">:</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">group_y_true</span><span class="p">,</span> <span class="n">group_y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'weighted'</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                            <span class="s">'positive_prediction_rate'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">group_y_pred</span><span class="p">),</span>
                            <span class="s">'size'</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">group_y_true</span><span class="p">)</span>
                        <span class="p">}</span>
                
                <span class="n">model_bias</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_metrics</span>
        
        <span class="k">return</span> <span class="n">model_bias</span>
    
    <span class="k">def</span> <span class="nf">visualize_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
        <span class="s">"""Create visualizations of bias"""</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sensitive_attributes</span><span class="p">[:</span><span class="mi">4</span><span class="p">]):</span>  <span class="c1"># Limit to 4 attributes
</span>            <span class="k">if</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">.</span><span class="n">columns</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">%</span><span class="mi">2</span><span class="p">]</span>
                
                <span class="c1"># Prediction rates by group
</span>                <span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
                    <span class="s">'Sensitive_Attribute'</span><span class="p">:</span> <span class="n">X_test</span><span class="p">[</span><span class="n">attr</span><span class="p">],</span>
                    <span class="s">'True_Label'</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span>
                    <span class="s">'Predicted_Label'</span><span class="p">:</span> <span class="n">predictions</span>
                <span class="p">})</span>
                
                <span class="n">prediction_rates</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'Sensitive_Attribute'</span><span class="p">)[</span><span class="s">'Predicted_Label'</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
                
                <span class="n">prediction_rates</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">'bar'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Positive Prediction Rate by </span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Positive Prediction Rate'</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">generate_bias_report</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
        <span class="s">"""Generate comprehensive bias report"""</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"BIAS DETECTION REPORT"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
        
        <span class="c1"># Dataset bias analysis
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">1. DATASET BIAS ANALYSIS"</span><span class="p">)</span>
        <span class="n">dataset_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">analyze_dataset_bias</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">attr</span><span class="p">,</span> <span class="n">analysis</span> <span class="ow">in</span> <span class="n">dataset_bias</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Attribute: </span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Distribution:"</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">value</span><span class="p">,</span> <span class="n">prop</span> <span class="ow">in</span> <span class="n">analysis</span><span class="p">[</span><span class="s">'distribution'</span><span class="p">].</span><span class="n">items</span><span class="p">():</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">prop</span><span class="p">:.</span><span class="mi">2</span><span class="o">%</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            
            <span class="k">print</span><span class="p">(</span><span class="s">"Outcome rates by group:"</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">value</span><span class="p">,</span> <span class="n">rate</span> <span class="ow">in</span> <span class="n">analysis</span><span class="p">[</span><span class="s">'outcome_rates'</span><span class="p">].</span><span class="n">items</span><span class="p">():</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">rate</span><span class="p">:.</span><span class="mi">2</span><span class="o">%</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        
        <span class="c1"># Model bias analysis
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">2. MODEL BIAS ANALYSIS"</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">model_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">analyze_model_bias</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">attr</span><span class="p">,</span> <span class="n">groups</span> <span class="ow">in</span> <span class="n">model_bias</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Attribute: </span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Performance by group:"</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">group</span><span class="p">,</span> <span class="n">metrics</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s"> (n=</span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s">'size'</span><span class="p">]</span><span class="si">}</span><span class="s">):"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"    Accuracy: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"    Precision: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s">'precision'</span><span class="p">]:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"    Recall: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s">'recall'</span><span class="p">]:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"    Positive Prediction Rate: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s">'positive_prediction_rate'</span><span class="p">]:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        
        <span class="c1"># Visualizations
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">3. BIAS VISUALIZATIONS"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">visualize_bias</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        
        <span class="c1"># Recommendations
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">4. RECOMMENDATIONS"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_generate_recommendations</span><span class="p">(</span><span class="n">dataset_bias</span><span class="p">,</span> <span class="n">model_bias</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_generate_recommendations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset_bias</span><span class="p">,</span> <span class="n">model_bias</span><span class="p">):</span>
        <span class="s">"""Generate actionable recommendations"""</span>
        <span class="n">recommendations</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Check for representation bias
</span>        <span class="k">for</span> <span class="n">attr</span><span class="p">,</span> <span class="n">analysis</span> <span class="ow">in</span> <span class="n">dataset_bias</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">distribution</span> <span class="o">=</span> <span class="n">analysis</span><span class="p">[</span><span class="s">'distribution'</span><span class="p">]</span>
            <span class="n">min_representation</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">distribution</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">min_representation</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="p">:</span>  <span class="c1"># Less than 10% representation
</span>                <span class="n">recommendations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s">"‚ö†Ô∏è Low representation for some groups in </span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s"> (min: </span><span class="si">{</span><span class="n">min_representation</span><span class="p">:.</span><span class="mi">1</span><span class="o">%</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>
                <span class="n">recommendations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s">"   ‚Üí Consider oversampling or collecting more data"</span><span class="p">)</span>
        
        <span class="c1"># Check for performance disparities
</span>        <span class="k">for</span> <span class="n">attr</span><span class="p">,</span> <span class="n">groups</span> <span class="ow">in</span> <span class="n">model_bias</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">accuracy_gap</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span> <span class="o">-</span> <span class="nb">min</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">accuracy_gap</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>  <span class="c1"># More than 5% accuracy gap
</span>                    <span class="n">recommendations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s">"‚ö†Ô∏è Performance gap in </span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">accuracy_gap</span><span class="p">:.</span><span class="mi">1</span><span class="o">%</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                    <span class="n">recommendations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s">"   ‚Üí Consider fairness-aware training or post-processing"</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">recommendations</span><span class="p">:</span>
            <span class="n">recommendations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">"‚úÖ No major bias issues detected"</span><span class="p">)</span>
            <span class="n">recommendations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">"   ‚Üí Continue monitoring and regular bias audits"</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">rec</span> <span class="ow">in</span> <span class="n">recommendations</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">rec</span><span class="p">)</span>

<span class="c1"># Example usage
</span><span class="k">def</span> <span class="nf">demonstrate_bias_detection</span><span class="p">():</span>
    <span class="s">"""Demonstrate the bias detection tool"""</span>
    
    <span class="c1"># Create synthetic biased dataset
</span>    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
    
    <span class="c1"># Generate features
</span>    <span class="n">age</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">35</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">gender</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="s">'M'</span><span class="p">,</span> <span class="s">'F'</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>  <span class="c1"># Gender imbalance
</span>    <span class="n">education</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="s">'High School'</span><span class="p">,</span> <span class="s">'College'</span><span class="p">,</span> <span class="s">'Graduate'</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">)</span>
    
    <span class="c1"># Create biased target (loan approval)
</span>    <span class="c1"># Bias: Lower approval rates for women
</span>    <span class="n">base_approval_prob</span> <span class="o">=</span> <span class="mf">0.6</span>
    <span class="n">gender_bias</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.2</span> <span class="k">if</span> <span class="s">'F'</span> <span class="k">else</span> <span class="mi">0</span>  <span class="c1"># This creates bias
</span>    
    <span class="n">approval_probs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">base_approval_prob</span>
        <span class="k">if</span> <span class="n">gender</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="s">'F'</span><span class="p">:</span>
            <span class="n">prob</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">0.2</span>  <span class="c1"># Bias against women
</span>        <span class="k">if</span> <span class="n">education</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="s">'Graduate'</span><span class="p">:</span>
            <span class="n">prob</span> <span class="o">+=</span> <span class="mf">0.3</span>
        <span class="k">elif</span> <span class="n">education</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="s">'College'</span><span class="p">:</span>
            <span class="n">prob</span> <span class="o">+=</span> <span class="mf">0.1</span>
            
        <span class="n">approval_probs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">prob</span><span class="p">)))</span>  <span class="c1"># Clamp between 0.1 and 0.9
</span>    
    <span class="n">approved</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">approval_probs</span><span class="p">)</span>
    
    <span class="c1"># Create DataFrame
</span>    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s">'age'</span><span class="p">:</span> <span class="n">age</span><span class="p">,</span>
        <span class="s">'gender'</span><span class="p">:</span> <span class="n">gender</span><span class="p">,</span>
        <span class="s">'education'</span><span class="p">:</span> <span class="n">education</span><span class="p">,</span>
        <span class="s">'approved'</span><span class="p">:</span> <span class="n">approved</span>
    <span class="p">})</span>
    
    <span class="c1"># Encode categorical variables
</span>    <span class="n">df_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'gender'</span><span class="p">,</span> <span class="s">'education'</span><span class="p">])</span>
    
    <span class="c1"># Split data
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">df_encoded</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'approved'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df_encoded</span><span class="p">[</span><span class="s">'approved'</span><span class="p">]</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="c1"># Train model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># Create bias detection tool
</span>    <span class="c1"># Note: Using original categorical columns for analysis
</span>    <span class="n">X_train_orig</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">X_train</span><span class="p">.</span><span class="n">index</span><span class="p">].</span><span class="n">drop</span><span class="p">(</span><span class="s">'approved'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test_orig</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">X_test</span><span class="p">.</span><span class="n">index</span><span class="p">].</span><span class="n">drop</span><span class="p">(</span><span class="s">'approved'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">bias_detector</span> <span class="o">=</span> <span class="n">BiasDetectionTool</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="s">'gender'</span><span class="p">,</span> <span class="s">'education'</span><span class="p">])</span>
    <span class="n">bias_detector</span><span class="p">.</span><span class="n">generate_bias_report</span><span class="p">(</span><span class="n">X_train_orig</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test_orig</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># Uncomment to run the demonstration
# demonstrate_bias_detection()
</span></code></pre></div></div>

<hr />

<h2 id="-global-perspectives-on-ai-ethics">üåç Global Perspectives on AI Ethics</h2>

<h3 id="european-union-gdpr-and-ai-act">European Union: GDPR and AI Act</h3>

<p><strong>GDPR (General Data Protection Regulation):</strong></p>
<ul>
  <li>Right to explanation for automated decision-making</li>
  <li>Data protection by design and by default</li>
  <li>Individual consent and control over data</li>
</ul>

<p><strong>Proposed AI Act:</strong></p>
<ul>
  <li>Risk-based regulation of AI systems</li>
  <li>Prohibited AI practices (social scoring, subliminal manipulation)</li>
  <li>High-risk system requirements (healthcare, criminal justice)</li>
</ul>

<h3 id="united-states-sectoral-approach">United States: Sectoral Approach</h3>

<p><strong>NIST AI Risk Management Framework:</strong></p>
<ul>
  <li>Voluntary guidelines for AI development</li>
  <li>Focus on risk management and mitigation</li>
  <li>Emphasis on trustworthy AI characteristics</li>
</ul>

<p><strong>Agency-Specific Regulations:</strong></p>
<ul>
  <li>FDA for medical AI</li>
  <li>EEOC for employment AI</li>
  <li>FTC for consumer protection</li>
</ul>

<h3 id="china-national-ai-strategy">China: National AI Strategy</h3>

<p><strong>Focus Areas:</strong></p>
<ul>
  <li>AI leadership and innovation</li>
  <li>Data governance and security</li>
  <li>Social stability and control</li>
</ul>

<p><strong>Unique Approaches:</strong></p>
<ul>
  <li>Social credit systems</li>
  <li>AI governance through party structure</li>
  <li>Emphasis on collective benefits</li>
</ul>

<hr />

<h2 id="-practical-frameworks-for-ethical-ai">üíº Practical Frameworks for Ethical AI</h2>

<h3 id="ethics-by-design-checklist">Ethics by Design Checklist</h3>

<p><strong>Before Development:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Define clear purpose and scope</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Identify potential stakeholders and impacts</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Assess data sources for bias</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Plan for transparency and explainability</li>
</ul>

<p><strong>During Development:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Test for bias across different groups</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement fairness metrics</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Document design decisions</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Include diverse perspectives in review</li>
</ul>

<p><strong>Before Deployment:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Conduct impact assessment</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Plan monitoring and evaluation</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Establish feedback mechanisms</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Train users on ethical considerations</li>
</ul>

<p><strong>After Deployment:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Monitor performance across groups</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Address identified problems promptly</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Regular bias audits</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Update as needed</li>
</ul>

<h3 id="stakeholder-impact-analysis">Stakeholder Impact Analysis</h3>

<p><strong>Identify Stakeholders:</strong></p>
<ul>
  <li>Direct users of the system</li>
  <li>People affected by system decisions</li>
  <li>Organizations implementing the system</li>
  <li>Society at large</li>
</ul>

<p><strong>Assess Impacts:</strong></p>
<ul>
  <li>Benefits and harms for each group</li>
  <li>Short-term and long-term effects</li>
  <li>Intended and unintended consequences</li>
  <li>Distribution of costs and benefits</li>
</ul>

<hr />

<h2 id="-curated-resources">üîó Curated Resources</h2>

<h3 id="essential-reading">Essential Reading</h3>
<ul>
  <li><a href="https://partnershiponai.org/">Partnership on AI Principles</a> - Industry best practices</li>
  <li><a href="https://ai-ethics-guidelines-inventory.com/">AI Ethics Guidelines Global Inventory</a> - Compare different frameworks</li>
  <li><a href="https://www.congress.gov/bill/117th-congress/house-bill/2231">Algorithmic Accountability Act</a> - Proposed US legislation</li>
</ul>

<h3 id="academic-papers">Academic Papers</h3>
<ul>
  <li><a href="https://dl.acm.org/doi/10.1145/3287560.3287598">‚ÄúFairness and Abstraction in Sociotechnical Systems‚Äù</a> - ACM FAccT</li>
  <li><a href="https://arxiv.org/abs/1607.06520">‚ÄúMan is to Computer Programmer as Woman is to Homemaker?‚Äù</a> - Word embedding bias</li>
  <li><a href="https://arxiv.org/abs/1610.02413">‚ÄúEqualizing Odds: Fairness Criteria for Machine Learning‚Äù</a> - Fairness definitions</li>
</ul>

<h3 id="video-resources">Video Resources</h3>
<ul>
  <li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP62uI5y_2wOJvKz3dn9kZYZf">AI Ethics Course - MIT</a></li>
  <li><a href="https://www.ajl.org/">Algorithmic Justice League</a> - Joy Buolamwini‚Äôs work on bias</li>
  <li><a href="https://www.youtube.com/watch?v=TQHs8SA1qpk">Weapons of Math Destruction</a> - Cathy O‚ÄôNeil‚Äôs talk</li>
</ul>

<h3 id="tools-and-frameworks">Tools and Frameworks</h3>
<ul>
  <li><a href="https://aif360.mybluemix.net/">IBM AI Fairness 360</a> - Open source bias detection toolkit</li>
  <li><a href="https://pair-code.github.io/what-if-tool/">Google What-If Tool</a> - Visual bias analysis</li>
  <li><a href="https://fairlearn.org/">Microsoft Fairlearn</a> - Python library for fairness assessment</li>
</ul>

<hr />

<h2 id="-key-principles-for-students">üéØ Key Principles for Students</h2>

<h3 id="as-future-ai-practitioners">As Future AI Practitioners</h3>

<ol>
  <li><strong>Stay Informed:</strong> Keep up with ethical developments in AI</li>
  <li><strong>Think Critically:</strong> Question assumptions and potential biases</li>
  <li><strong>Collaborate:</strong> Work with diverse teams and stakeholders</li>
  <li><strong>Be Transparent:</strong> Document decisions and trade-offs</li>
  <li><strong>Take Responsibility:</strong> Consider the broader impact of your work</li>
</ol>

<h3 id="questions-to-always-ask">Questions to Always Ask</h3>

<ul>
  <li><strong>Who benefits</strong> from this AI system?</li>
  <li><strong>Who might be harmed</strong> by this system?</li>
  <li><strong>What biases</strong> might exist in the data or algorithm?</li>
  <li><strong>How transparent</strong> is the decision-making process?</li>
  <li><strong>What safeguards</strong> are in place?</li>
  <li><strong>How will we know</strong> if the system is working as intended?</li>
</ul>

<hr />

<h2 id="-discussion-questions">ü§î Discussion Questions</h2>

<ol>
  <li>
    <p>Can AI systems ever be truly ‚Äúobjective‚Äù or are they always reflections of human biases?</p>
  </li>
  <li>
    <p>Should there be a ‚Äúright to explanation‚Äù for all AI decisions that affect individuals?</p>
  </li>
  <li>
    <p>How do we balance the benefits of AI surveillance (crime prevention, public health) with privacy rights?</p>
  </li>
  <li>
    <p>What responsibility do tech companies have for the societal impacts of their AI systems?</p>
  </li>
  <li>
    <p>How can we ensure that AI benefits everyone, not just those with access to technology and data?</p>
  </li>
  <li>
    <p>What role should the public play in governing AI development and deployment?</p>
  </li>
</ol>

<hr />

<h2 id="-assignment-ethical-ai-analysis">üìö Assignment: Ethical AI Analysis</h2>

<h3 id="individual-reflection-required">Individual Reflection (Required)</h3>
<p>Write a 2-page reflection addressing:</p>

<ol>
  <li><strong>Issue Identification:</strong> Choose a specific AI ethical issue that concerns you most and explain why</li>
  <li><strong>Personal Impact:</strong> How might this issue affect you or your community?</li>
  <li><strong>Professional Response:</strong> How would you address this issue as an AI practitioner?</li>
  <li><strong>Policy Recommendations:</strong> What policies or practices would you recommend?</li>
  <li><strong>Future Considerations:</strong> How might this issue evolve as AI technology advances?</li>
</ol>

<h3 id="group-project-optional">Group Project (Optional)</h3>
<p>Choose a current AI system and conduct an ethical analysis:</p>

<ol>
  <li><strong>System Overview:</strong> Describe the AI system and its purpose</li>
  <li><strong>Stakeholder Analysis:</strong> Identify all affected parties</li>
  <li><strong>Bias Assessment:</strong> Identify potential biases and their sources</li>
  <li><strong>Fairness Evaluation:</strong> Assess fairness across different groups</li>
  <li><strong>Recommendations:</strong> Propose improvements and safeguards</li>
  <li><strong>Presentation:</strong> Present findings to the class (10 minutes + Q&amp;A)</li>
</ol>

<p><strong>Suggested Systems to Analyze:</strong></p>
<ul>
  <li>Hiring algorithms (LinkedIn, ZipRecruiter)</li>
  <li>Credit scoring systems (FICO, alternative credit)</li>
  <li>Content moderation (Facebook, YouTube)</li>
  <li>Predictive policing (PredPol, CompStat)</li>
  <li>Healthcare AI (diagnostic imaging, drug discovery)</li>
</ul>

<hr />

<h2 id="-looking-ahead">üîÆ Looking Ahead</h2>

<p>Next week, we‚Äôll conclude our AI journey with <strong>course review, project presentations, and exploring the future of AI</strong>. We‚Äôll synthesize everything we‚Äôve learned and discuss emerging trends and challenges.</p>

<p><strong>Preview of Week 14:</strong></p>
<ul>
  <li>Course review and concept integration</li>
  <li>Student project presentations</li>
  <li>Emerging AI technologies and trends</li>
  <li>Career paths in AI</li>
  <li>The future of artificial intelligence</li>
</ul>

<hr />

<h3 id="navigation-1">Navigation</h3>
<p><a href="index.html">üè† Home</a> | <a href="week12.html">‚Üê Previous: Week 12</a> | <a href="week14.html">Next: Week 14 ‚Üí</a></p>

<hr />

<p><em>‚ÄúWith great power comes great responsibility. As AI becomes more powerful, our responsibility to develop and deploy it ethically becomes even greater.‚Äù</em></p>

<p><strong>Remember:</strong> Ethics in AI isn‚Äôt about having all the answers‚Äîit‚Äôs about asking the right questions and being committed to addressing the challenges we discover.</p>

</div>

<style>
.week-header {
  margin: -2rem -1.5rem 3rem -1.5rem;
  padding: 2rem 1.5rem;
  background: linear-gradient(135deg, rgba(99, 102, 241, 0.08) 0%, rgba(139, 92, 246, 0.08) 100%);
  border-radius: 0 0 1.5rem 1.5rem;
}

.week-hero {
  display: flex;
  align-items: center;
  gap: 2rem;
  max-width: var(--container-max-width);
  margin: 0 auto;
}

.week-number-large {
  font-size: 4rem;
  font-weight: 800;
  background: var(--primary-gradient);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
  opacity: 0.8;
  min-width: 5rem;
  text-align: center;
}

.week-title-section {
  flex: 1;
}

.week-title-section h1 {
  margin-bottom: 1rem;
  font-size: 2rem;
}

.week-progress-bar {
  width: 100%;
  height: 8px;
  background: rgba(255, 255, 255, 0.3);
  border-radius: 4px;
  overflow: hidden;
}

.week-progress-bar .progress-fill {
  height: 100%;
  background: var(--primary-gradient);
  border-radius: 4px;
  transition: width 0.8s ease;
}

.week-navigation {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-top: 2rem;
  max-width: var(--container-max-width);
  margin-left: auto;
  margin-right: auto;
}

.week-content {
  animation: fadeInUp 0.6s ease-out;
}

.learning-objectives {
  background: linear-gradient(135deg, rgba(99, 102, 241, 0.05), rgba(139, 92, 246, 0.05));
  border: 1px solid rgba(99, 102, 241, 0.1);
  border-left: 4px solid var(--primary-color);
  padding: 2rem;
  border-radius: var(--border-radius);
  margin: 2rem 0;
  box-shadow: var(--shadow);
}

.learning-objectives h2 {
  color: var(--primary-color);
  margin-bottom: 1.5rem;
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

.learning-objectives ul {
  list-style: none;
  padding: 0;
}

.learning-objectives li {
  position: relative;
  padding-left: 2rem;
  margin-bottom: 1rem;
  color: var(--text-secondary);
}

.learning-objectives li::before {
  content: '‚úì';
  position: absolute;
  left: 0;
  color: var(--accent-3);
  font-weight: bold;
  font-size: 1.2rem;
}

@media (max-width: 768px) {
  .week-header {
    margin: -2rem -1rem 2rem -1rem;
    padding: 1.5rem 1rem;
  }
  
  .week-hero {
    flex-direction: column;
    text-align: center;
    gap: 1rem;
  }
  
  .week-number-large {
    font-size: 3rem;
  }
  
  .week-title-section h1 {
    font-size: 1.5rem;
  }
  
  .week-navigation {
    flex-direction: column;
    gap: 1rem;
  }
}
</style>
      </div>
    </main>

    <!-- Footer -->
    <footer class="site-footer">
      <div class="footer-container">
        <div class="footer-content">
          <div class="footer-section">
            <h3>AI Learning Students</h3>
            <p>A comprehensive educational resource for learning Artificial Intelligence - structured 14-week curriculum for undergraduate students</p>
          </div>
          <div class="footer-section">
            <h4>Quick Links</h4>
            <ul>
              <li><a href="/ai-learning-students/">Course Overview</a></li>
              <li><a href="https://github.com/irfan-sec/ai-learning-students">GitHub Repository</a></li>
              <li><a href="/ai-learning-students/week01.html">Get Started</a></li>
            </ul>
          </div>
          <div class="footer-section">
            <h4>Resources</h4>
            <ul>
              <li><a href="https://aima.cs.berkeley.edu/">AIMA Textbook</a></li>
              <li><a href="https://scikit-learn.org/">Scikit-learn</a></li>
              <li><a href="https://www.fast.ai/">Fast.ai</a></li>
            </ul>
          </div>
        </div>
        <div class="footer-bottom">
          <p>&copy; 2024 AI Learning Community. Licensed under MIT License.</p>
        </div>
      </div>
    </footer>
  </div>

  <!-- Scripts -->
  <script src="/ai-learning-students/assets/js/theme.js"></script>
</body>
</html>